{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Append the library path to PYTHONPATH, so library can be imported.\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from library import plot, bs\n",
    "from library import network as nw\n",
    "from library import common as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run setup.py\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%run Load_Clean_aux.py normal\n",
    "\n",
    "seed = 666\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FEATURE_SET == 'normal_feature':\n",
    "    ori_fea = ['M0', 'tau0_implvol0']\n",
    "    sub_res = res_dir + 'Network/Normal_Feature/'\n",
    "\n",
    "if FEATURE_SET == 'delta_vega':\n",
    "    ori_fea = ['delta_bs', '1_over_sqrt_tau', 'vega_n']\n",
    "    sub_res = res_dir + 'Network/Delta_Vega/'\n",
    "    \n",
    "if VIX:\n",
    "    ori_fea += ['fake_vix']\n",
    "\n",
    "    \n",
    "os.makedirs(sub_res, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers = {\n",
    "    'nodes_per_layer': (30, 30),\n",
    "    'reg_alpha': 1e-3,\n",
    "    'lr': 1e-4,\n",
    "    'epochs': 500, #1000\n",
    "    'outact': 'linear'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note here, we use the same directory structure for the permuted and non-permuted data. \n",
    "For the non-permuted data:\n",
    "    The network is only trained once. so the ckp and the history are from that training.\n",
    "    Each of the pnls is for each different monte carlo, but they come from the same network.\n",
    "    \n",
    "For the permuted data:\n",
    "    The network is trained for the number of permutations. \n",
    "    Each of the pnls is for each permuations, and they comes from each trained network.\n",
    "\"\"\"\n",
    "\n",
    "sub_res_dirs = {\n",
    "    'ckp': sub_res + 'ckp/',\n",
    "    'history': sub_res + 'history/',\n",
    "    'pnl': sub_res + 'pnl/',\n",
    "    'plot': sub_res + 'plot/'\n",
    "}\n",
    "for key, value in sub_res_dirs.items():\n",
    "    os.makedirs(value, exist_ok=True)\n",
    "shutil.copy('setup.py', sub_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not PERMUTE:\n",
    "\n",
    "    # The loaded `df_train` set contains both the training and the validation set. So we need to split.\n",
    "    df_val = df_train.loc[df_train['period0'] == 1]\n",
    "    df_train = df_train.loc[df_train['period0'] == 0]\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ##### Step 2: Choose feature and standardize\n",
    "    Before data sets are fed to a network, all their features need to be standardized to \n",
    "    have zero mean and unit standard deviation.\n",
    "    \"\"\"\n",
    "    use_fea = [x + '_t' for x in ori_fea] + ['cp_int']\n",
    "\n",
    "    scaler = StandardScaler().fit(X=df_train[ori_fea])\n",
    "    df_train, df_val = nw.standardize_feature([df_train, df_val], scaler, ori_fea)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    ##### Step 3: Build a network and train it\n",
    "    \"\"\"\n",
    "    sub_res_paths = {\n",
    "        'ckp': sub_res_dirs['ckp'] + 'bestcp.h5',\n",
    "        'history': sub_res_dirs['history'] + 'history.csv',\n",
    "        'plot': sub_res_dirs['plot'] + 'losscurve.png'\n",
    "    }\n",
    "    history = nw.train_net_core(df_train, df_val, use_fea, hypers, sub_res_paths)    \n",
    "    nw.plot_history(history, sub_res_paths['plot'], df_train, df_val)\n",
    "    \n",
    "    for i in range(NUM_TEST):\n",
    "        df_test = mc_sets[i]\n",
    "\n",
    "        [df_test] = nw.standardize_feature([df_test], scaler, ori_fea)\n",
    "        delta = nw.test_net_core(df_test, use_fea, sub_res_paths)\n",
    "    \n",
    "        cm.store_pnl(\n",
    "            df_test, delta,\n",
    "            pnl_path=sub_res_dirs['pnl'] + f'pnl{i}.csv'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prepare permutations.\n",
    "\"\"\"\n",
    "if PERMUTE:\n",
    "    train_permutes, val_permutes, test_permutes = [], [], []\n",
    "    for i in range(NUM_TEST):\n",
    "        # the union of train and test\n",
    "        df_permute = df_train.append(mc_sets[i], ignore_index=True, sort=False)\n",
    "        df_permute = cm.permute_core(df_permute, 0, random_seed=i)\n",
    "\n",
    "        df_train_permuted = df_permute.loc[df_permute['period0'] == 0]\n",
    "        df_val_permuted = df_permute.loc[df_permute['period0'] == 1]\n",
    "        df_test_permuted = df_permute.loc[df_permute['period0'] == 2]\n",
    "\n",
    "        train_permutes.append(df_train_permuted.copy())\n",
    "        val_permutes.append(df_val_permuted.copy())\n",
    "        test_permutes.append(df_test_permuted.copy())\n",
    "    del mc_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if PERMUTE:\n",
    "    use_fea = [x + '_t' for x in ori_fea] + ['cp_int']\n",
    "    for i in range(NUM_TEST):\n",
    "        \"\"\"\n",
    "        ##### Step 2: Choose feature and standardize\n",
    "        The difference of permuating version and the above version is:\n",
    "        we standardize for each permutation.\n",
    "        \"\"\"\n",
    "        scaler = StandardScaler().fit(X=train_permutes[i][ori_fea])\n",
    "        train_permutes[i], val_permutes[i] = nw.standardize_feature([train_permutes[i], val_permutes[i]], scaler, ori_fea)\n",
    "        \n",
    "        \"\"\"\n",
    "        ##### Step 3: Build a network and train it\n",
    "        \"\"\"\n",
    "        sub_res_paths = {\n",
    "            'ckp': sub_res_dirs['ckp'] + f'bestcp{i}.h5',\n",
    "            'history': sub_res_dirs['history'] + f'history{i}.csv',\n",
    "            'plot': sub_res_dirs['plot'] + f'losscurve{i}.png'\n",
    "        }\n",
    "        history = nw.train_net_core(train_permutes[i], val_permutes[i], use_fea, hypers, sub_res_paths)\n",
    "        nw.plot_history(history, sub_res_paths['plot'], train_permutes[i], val_permutes[i])\n",
    "\n",
    "        \"\"\"\n",
    "        Test the network for only one permuted test set.\n",
    "        \"\"\"\n",
    "        [test_permutes[i]] = nw.standardize_feature([test_permutes[i]], scaler, ori_fea)\n",
    "        delta = nw.test_net_core(test_permutes[i], use_fea, sub_res_paths)\n",
    "        cm.store_pnl(\n",
    "            test_permutes[i], delta,\n",
    "            pnl_path=sub_res_dirs['pnl'] + f'pnl{i}.csv'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(f'{sub_res}additional_paras.txt', 'w+') as file:\n",
    "    file.write('The following is network setup.\\n')\n",
    "    file.write(f'Date and time = {datetime.datetime.now()}\\n')\n",
    "    for n, x in [\n",
    "        ('Random seed', seed),\n",
    "        ('Features used', use_fea),\n",
    "        ('Learning rate', hypers['lr']),\n",
    "        ('L2 regularization alpha', hypers['reg_alpha']),\n",
    "        ('Nodes per layer', hypers['nodes_per_layer']),\n",
    "        ('Number of training epochs', hypers['epochs'])\n",
    "    ]:\n",
    "        file.write(f'{n} = {x}\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
